package rabbitmq

import (
	"context"
	"encoding/json"
	"errors"
	"fmt"
	"io/ioutil"
	"math/rand"
	"sync"
	"sync/atomic"
	"testing"
	"time"

	"github.com/godaddy-x/freego/utils"
	"github.com/streadway/amqp"
	"github.com/stretchr/testify/assert"
)

// setupTestPullManager åˆ›å»ºæµ‹è¯•ç”¨çš„æ¶ˆè´¹ç®¡ç†å™¨
func setupTestPullManager(t *testing.T, dsName string) *PullManager {
	conf := loadTestConfig(t)
	conf.DsName = dsName

	mgr, err := NewPull(dsName)
	if err != nil {
		t.Fatalf("Failed to create pull manager: %v", err)
	}
	return mgr
}

func TestNewPull(t *testing.T) {
	// æ¸…ç†å…¨å±€çŠ¶æ€
	pullMgrMu.Lock()
	pullMgrs = make(map[string]*PullManager)
	pullMgrMu.Unlock()

	// æµ‹è¯•æ­£å¸¸åˆ›å»º
	mgr, err := NewPull("test")
	assert.NoError(t, err)
	assert.NotNil(t, mgr)

	// æµ‹è¯•é»˜è®¤æ•°æ®æº
	mgr2, err := NewPull()
	assert.NoError(t, err)
	assert.NotNil(t, mgr2)

	// æ¸…ç†
	pullMgrMu.Lock()
	delete(pullMgrs, "test")
	delete(pullMgrs, "master")
	pullMgrMu.Unlock()
}

func TestPullReceiver_initDefaults(t *testing.T) {
	receiver := &PullReceiver{}

	// æµ‹è¯•é»˜è®¤å€¼è®¾ç½®
	receiver.initDefaults()

	assert.NotNil(t, receiver.Config)
	assert.Equal(t, 50, receiver.Config.PrefetchCount) // æ–°çš„é»˜è®¤å€¼
	assert.Equal(t, 0, receiver.Config.Option.SigTyp)  // åˆå§‹å€¼ä¸º0ï¼Œåªæœ‰å½“å€¼ä¸åœ¨0-1èŒƒå›´å†…æ—¶æ‰è®¾ä¸º1
}

func TestPullReceiver_initControlChans(t *testing.T) {
	receiver := &PullReceiver{}

	// æµ‹è¯•æ§åˆ¶é€šé“åˆå§‹åŒ–
	receiver.initControlChans()

	assert.NotNil(t, receiver.closeChan)
	assert.NotNil(t, receiver.stopChan)
	assert.False(t, receiver.stopping)
	assert.True(t, receiver.healthy)
}

func TestParseMessage(t *testing.T) {
	receiver := &PullReceiver{
		Config: &Config{},
	}

	// æµ‹è¯•æ­£å¸¸æ¶ˆæ¯è§£æ
	msgData := &MsgData{
		Content:   "test content",
		Nonce:     "test nonce",
		Signature: "test signature",
	}

	data, _ := json.Marshal(msgData)
	parsedMsg := GetMsgData()
	defer PutMsgData(parsedMsg)

	err := receiver.parseMessage(data, parsedMsg)

	assert.NoError(t, err)
	assert.NotNil(t, parsedMsg)
	assert.Equal(t, "test content", parsedMsg.Content)
	assert.Equal(t, "test nonce", parsedMsg.Nonce)
	assert.Equal(t, "test signature", parsedMsg.Signature)
}

func TestParseMessage_InvalidJSON(t *testing.T) {
	receiver := &PullReceiver{
		Config: &Config{},
	}

	// æµ‹è¯•æ— æ•ˆJSON
	parsedMsg := GetMsgData()
	defer PutMsgData(parsedMsg)

	err := receiver.parseMessage([]byte("invalid json"), parsedMsg)

	assert.Error(t, err)
	assert.Contains(t, err.Error(), "json unmarshal failed")
}

func TestValidateAESKeyLength(t *testing.T) {
	tests := []struct {
		name      string
		key       string
		expectErr bool
		errMsg    string
	}{
		{"empty key", "", true, "AES key cannot be empty"},
		{"key too short", "short", true, "AES key too short"},
		{"key too long", string(make([]byte, 129)), true, "AES key too long"},
		{"AES-128", string(make([]byte, 16)), false, ""},
		{"AES-192", string(make([]byte, 24)), false, ""},
		{"AES-256", string(make([]byte, 32)), false, ""},
	}

	for _, tt := range tests {
		t.Run(tt.name, func(t *testing.T) {
			err := validateAESKeyLength(tt.key)
			if tt.expectErr {
				assert.Error(t, err)
				assert.Contains(t, err.Error(), tt.errMsg)
			} else {
				assert.NoError(t, err)
			}
		})
	}
}

func TestValidateMessage_NoSignature(t *testing.T) {
	receiver := &PullReceiver{
		Config: &Config{},
	}

	msg := &MsgData{
		Content: "test",
		Nonce:   "test",
	}

	err := receiver.validateMessage(msg)
	assert.Error(t, err)
	assert.Contains(t, err.Error(), "message signature is empty")
}

func TestValidateMessage_SignatureVerification(t *testing.T) {
	receiver := &PullReceiver{
		Config: &Config{
			Option: Option{
				SigTyp: 0,                           // ä¸åŠ å¯†æ¨¡å¼
				SigKey: "test_key_1234567890123456", // 16å­—èŠ‚AESå¯†é’¥
			},
		},
	}

	// åˆ›å»ºæµ‹è¯•æ¶ˆæ¯
	content := "test content"
	nonce := "test nonce"
	combined := utils.AddStr(content, nonce)
	signature := utils.HMAC_SHA256(combined, receiver.Config.Option.SigKey, true)

	msg := &MsgData{
		Content:   content,
		Nonce:     nonce,
		Signature: signature,
	}

	// æµ‹è¯•ç­¾åéªŒè¯æˆåŠŸ
	err := receiver.validateMessage(msg)
	assert.NoError(t, err)
}

func TestValidateMessage_InvalidSignature(t *testing.T) {
	receiver := &PullReceiver{
		Config: &Config{
			Option: Option{
				SigTyp: 0,
				SigKey: "test_key_1234567890123456",
			},
		},
	}

	msg := &MsgData{
		Content:   "test content",
		Nonce:     "test nonce",
		Signature: "invalid_signature",
	}

	err := receiver.validateMessage(msg)
	assert.Error(t, err)
	assert.Contains(t, err.Error(), "signature verification failed")
}

func TestValidateMessage_AESCBCDecrypt(t *testing.T) {
	receiver := &PullReceiver{
		Config: &Config{
			Option: Option{
				SigTyp: 1,                           // åŠ å¯†æ¨¡å¼
				SigKey: "test_key_1234567890123456", // 16å­—èŠ‚AESå¯†é’¥
			},
		},
	}

	// å…ˆåŠ å¯†å†…å®¹
	plainContent := "test content"
	encryptedContent, err := utils.AesGCMEncrypt(utils.Str2Bytes(plainContent), receiver.Config.Option.SigKey)
	assert.NoError(t, err)

	// åˆ›å»ºæ¶ˆæ¯ï¼ˆåŸºäºåŠ å¯†å†…å®¹ç”Ÿæˆç­¾åï¼‰
	nonce := "test nonce"
	combined := utils.AddStr(encryptedContent, nonce)
	signature := utils.HMAC_SHA256(combined, receiver.Config.Option.SigKey, true)

	msg := &MsgData{
		Content:   encryptedContent,
		Nonce:     nonce,
		Signature: signature,
	}

	// æµ‹è¯•è§£å¯†å’ŒéªŒè¯
	err = receiver.validateMessage(msg)
	assert.NoError(t, err)
	assert.Equal(t, plainContent, msg.Content) // å†…å®¹åº”è¯¥è¢«è§£å¯†
}

func TestIsHealthy(t *testing.T) {
	receiver := &PullReceiver{}

	// æµ‹è¯•åˆå§‹çŠ¶æ€
	assert.False(t, receiver.IsHealthy())

	// è®¾ç½®å¥åº·çŠ¶æ€
	receiver.healthy = true
	receiver.channel = &amqp.Channel{} // mock channel

	assert.True(t, receiver.IsHealthy())
}

func TestPullReceiver_Stop(t *testing.T) {
	receiver := &PullReceiver{}

	// åˆå§‹åŒ–æ§åˆ¶é€šé“
	receiver.initControlChans()

	// æµ‹è¯•åœæ­¢
	receiver.Stop()

	assert.True(t, receiver.stopping)
	assert.False(t, receiver.healthy)
	assert.Nil(t, receiver.channel)
}

// åŸºå‡†æµ‹è¯•
func BenchmarkParseMessage(b *testing.B) {
	receiver := &PullReceiver{
		Config: &Config{},
	}

	msgData := &MsgData{
		Content:   "test content",
		Nonce:     "test nonce",
		Signature: "test signature",
	}
	data, _ := json.Marshal(msgData)

	b.ResetTimer()
	for i := 0; i < b.N; i++ {
		msg := GetMsgData()
		receiver.parseMessage(data, msg)
		PutMsgData(msg)
	}
}

func BenchmarkValidateMessage(b *testing.B) {
	receiver := &PullReceiver{
		Config: &Config{
			Option: Option{
				SigTyp: 0,
				SigKey: "test_key_1234567890123456",
			},
		},
	}

	msg := &MsgData{
		Content:   "test content",
		Nonce:     "test nonce",
		CreatedAt: 1234567890,
		Signature: utils.HMAC_SHA256(utils.AddStr("test content", "test nonce", int64(1234567890)), "test_key_1234567890123456", true),
	}

	b.ResetTimer()
	for i := 0; i < b.N; i++ {
		receiver.validateMessage(msg)
	}
}

func TestRealEnvironmentPull1(t *testing.T) {
	receiver := &PullReceiver{
		Config: &Config{
			Option: Option{
				Exchange: "test.exchange",
				Queue:    "test.queue",
				Router:   "test.key",
				SigKey:   "rabbitmq_secret_key_32_bytes_1234567890", // è®¾ç½®ç­¾åå¯†é’¥ï¼Œä¸é…ç½®æ–‡ä»¶ä¸€è‡´
				Durable:  true,                                      // ä½¿ç”¨éæŒä¹…åŒ–ä»¥é¿å…å‚æ•°å†²çª
			},
			Exclusive: true,
			IsNack:    true,
		},
		Callback: func(msg *MsgData) error {
			// è§£ææ¶ˆæ¯å†…å®¹
			//var content map[string]interface{}
			//if err := json.Unmarshal([]byte(msg.Content), &content); err != nil {
			//	t.Errorf("Failed to parse message content: %v", err)
			//	return err
			//}
			//
			//messagesMutex.Lock()
			//receivedMessages = append(receivedMessages, content)
			//messagesMutex.Unlock()

			t.Logf("Received message: %v", msg.Content)
			return nil
		},
	}

	// åŠ è½½RabbitMQé…ç½®æ–‡ä»¶
	configData, err := ioutil.ReadFile("../resource/rabbitmq.json")
	if err != nil {
		t.Fatalf("Failed to read RabbitMQ config file: %v", err)
	}

	var conf AmqpConfig
	if err := json.Unmarshal(configData, &conf); err != nil {
		t.Fatalf("Failed to parse RabbitMQ config: %v", err)
	}

	// åˆå§‹åŒ–æ¶ˆè´¹ç®¡ç†å™¨
	mgr := &PullManager{}
	if err := mgr.InitConfig(conf); err != nil {
		t.Fatalf("Failed to init pull manager: %v", err)
	}
	defer func() {
		if err := mgr.Close(); err != nil {
			t.Logf("Error closing manager: %v", err)
		}
	}()
	mgr.AddPullReceiver(receiver)
	select {}
}

// TestRealEnvironmentPull å®é™…ç¯å¢ƒæ¶ˆè´¹æ¶ˆæ¯æµ‹è¯•
func TestRealEnvironmentPull(t *testing.T) {
	// åŠ è½½RabbitMQé…ç½®æ–‡ä»¶
	configData, err := ioutil.ReadFile("../resource/rabbitmq.json")
	if err != nil {
		t.Fatalf("Failed to read RabbitMQ config file: %v", err)
	}

	var conf AmqpConfig
	if err := json.Unmarshal(configData, &conf); err != nil {
		t.Fatalf("Failed to parse RabbitMQ config: %v", err)
	}

	// åˆå§‹åŒ–æ¶ˆè´¹ç®¡ç†å™¨
	mgr := &PullManager{}
	if err := mgr.InitConfig(conf); err != nil {
		t.Fatalf("Failed to init pull manager: %v", err)
	}
	defer func() {
		if err := mgr.Close(); err != nil {
			t.Logf("Error closing manager: %v", err)
		}
	}()

	// ç”¨äºæ”¶é›†æ¥æ”¶åˆ°çš„æ¶ˆæ¯
	var receivedMessages []map[string]interface{}
	var messagesMutex sync.Mutex
	var wg sync.WaitGroup

	// åˆ›å»ºæµ‹è¯•æ¥æ”¶å™¨
	receiver := &PullReceiver{
		Config: &Config{
			Option: Option{
				Exchange: "test.exchange",
				Queue:    "test.queue",
				Router:   "test.key",
				SigKey:   "rabbitmq_secret_key_32_bytes_1234567890", // è®¾ç½®ç­¾åå¯†é’¥ï¼Œä¸é…ç½®æ–‡ä»¶ä¸€è‡´
				Durable:  true,                                      // ä½¿ç”¨éæŒä¹…åŒ–ä»¥é¿å…å‚æ•°å†²çª
			},
			IsNack: true,
		},
		Callback: func(msg *MsgData) error {
			// è§£ææ¶ˆæ¯å†…å®¹
			//var content map[string]interface{}
			//if err := json.Unmarshal([]byte(msg.Content), &content); err != nil {
			//	t.Errorf("Failed to parse message content: %v", err)
			//	return err
			//}
			//
			//messagesMutex.Lock()
			//receivedMessages = append(receivedMessages, content)
			//messagesMutex.Unlock()

			t.Logf("Received message: %v", msg.Content)
			return errors.New("=====")
		},
	}

	// åˆå§‹åŒ–æ¥æ”¶å™¨
	receiver.initDefaults()
	receiver.initControlChans()

	// æ·»åŠ æ¥æ”¶å™¨åˆ°ç®¡ç†å™¨
	err = mgr.AddPullReceiver(receiver)
	if err != nil {
		t.Fatalf("Failed to add receiver: %v", err)
	}

	ctx := context.Background()

	t.Run("SingleMessageConsumption", func(t *testing.T) {
		// å¯åŠ¨goroutineå‘é€æµ‹è¯•æ¶ˆæ¯
		wg.Add(1)
		go func() {
			defer wg.Done()

			// ç­‰å¾…æ¶ˆè´¹è€…å¯åŠ¨
			time.Sleep(2 * time.Second)

			// åˆ›å»ºå‘å¸ƒç®¡ç†å™¨å‘é€æ¶ˆæ¯
			pubMgr, err := NewPublishManager(conf)
			if err != nil {
				t.Errorf("Failed to create publish manager: %v", err)
				return
			}
			defer pubMgr.Close()

			testData := map[string]interface{}{
				"id":          fmt.Sprintf("pull-test-%d", time.Now().Unix()),
				"message":     "Hello from real environment pull test!",
				"type":        "single",
				"timestamp":   time.Now().Unix(),
				"environment": "test",
			}
			testDataBytes, _ := json.Marshal(testData)

			err = pubMgr.Publish(ctx, "test.exchange", "test.queue.pull", 1, string(testDataBytes),
				WithRouter("test.routing.key"), WithDurable(true))
			if err != nil {
				t.Errorf("Failed to publish test message: %v", err)
				return
			}

			t.Logf("Successfully published test message: %v", testData)
		}()

		// ç­‰å¾…æ¶ˆæ¯è¢«æ¶ˆè´¹
		timeout := time.After(10 * time.Second)
		ticker := time.NewTicker(500 * time.Millisecond)
		defer ticker.Stop()

		for {
			select {
			case <-timeout:
				t.Error("Timeout waiting for message consumption")
				return
			case <-ticker.C:
				messagesMutex.Lock()
				if len(receivedMessages) > 0 {
					messagesMutex.Unlock()
					goto messageReceived
				}
				messagesMutex.Unlock()
			}
		}

	messageReceived:
		// éªŒè¯æ¥æ”¶åˆ°çš„æ¶ˆæ¯
		messagesMutex.Lock()
		if len(receivedMessages) == 0 {
			t.Error("No messages were received")
		} else {
			msg := receivedMessages[0]
			assert.Equal(t, "single", msg["type"])
			assert.Contains(t, msg, "id")
			assert.Contains(t, msg, "message")
			t.Logf("Successfully consumed message: %v", msg)
		}
		messagesMutex.Unlock()
	})

	t.Run("BatchMessageConsumption", func(t *testing.T) {
		// æ¸…ç©ºä¹‹å‰çš„æ¶ˆæ¯
		messagesMutex.Lock()
		receivedMessages = nil
		messagesMutex.Unlock()

		// å¯åŠ¨goroutineå‘é€æ‰¹é‡æµ‹è¯•æ¶ˆæ¯
		wg.Add(1)
		go func() {
			defer wg.Done()

			// ç­‰å¾…æ¶ˆè´¹è€…å‡†å¤‡
			time.Sleep(1 * time.Second)

			// åˆ›å»ºå‘å¸ƒç®¡ç†å™¨å‘é€æ‰¹é‡æ¶ˆæ¯
			pubMgr, err := NewPublishManager(conf)
			if err != nil {
				t.Errorf("Failed to create publish manager: %v", err)
				return
			}
			defer pubMgr.Close()

			batchSize := 3
			msgs := make([]*MsgData, batchSize)
			for i := 0; i < batchSize; i++ {
				contentData := map[string]interface{}{
					"id":          fmt.Sprintf("batch-pull-%d-%d", i+1, time.Now().Unix()),
					"message":     fmt.Sprintf("Batch pull message %d", i+1),
					"type":        "batch",
					"batch_index": i + 1,
					"timestamp":   time.Now().Unix(),
				}
				contentBytes, _ := json.Marshal(contentData)
				msgs[i] = &MsgData{
					Content: string(contentBytes),
					Option: Option{
						Exchange: "test.exchange",
						Queue:    "test.queue.pull",
						Router:   "test.routing.key",
						SigKey:   "rabbitmq_secret_key_32_bytes_1234567890", // è®¾ç½®ç­¾åå¯†é’¥
						Durable:  true,
					},
					Type: 1,
				}
			}

			err = pubMgr.BatchPublishWithOptions(ctx, msgs, WithSigType(0))
			if err != nil {
				t.Errorf("Failed to publish batch messages: %v", err)
				return
			}

			t.Logf("Successfully published %d batch messages", batchSize)
		}()

		// ç­‰å¾…æ‰€æœ‰æ¶ˆæ¯è¢«æ¶ˆè´¹
		timeout := time.After(15 * time.Second)
		expectedCount := 3

		for {
			select {
			case <-timeout:
				messagesMutex.Lock()
				actualCount := len(receivedMessages)
				messagesMutex.Unlock()
				t.Errorf("Timeout waiting for batch messages. Expected: %d, Received: %d", expectedCount, actualCount)
				return
			default:
				messagesMutex.Lock()
				if len(receivedMessages) >= expectedCount {
					messagesMutex.Unlock()
					goto batchReceived
				}
				messagesMutex.Unlock()
				time.Sleep(500 * time.Millisecond)
			}
		}

	batchReceived:
		// éªŒè¯æ¥æ”¶åˆ°çš„æ‰¹é‡æ¶ˆæ¯
		messagesMutex.Lock()
		if len(receivedMessages) < expectedCount {
			t.Errorf("Expected %d messages, got %d", expectedCount, len(receivedMessages))
		} else {
			t.Logf("Successfully consumed %d batch messages", len(receivedMessages))
			for i, msg := range receivedMessages {
				assert.Equal(t, "batch", msg["type"])
				assert.Contains(t, msg, "batch_index")
				t.Logf("Batch message %d: %v", i+1, msg)
			}
		}
		messagesMutex.Unlock()
	})

	t.Run("EncryptedMessageConsumption", func(t *testing.T) {
		// æ¸…ç©ºä¹‹å‰çš„æ¶ˆæ¯
		messagesMutex.Lock()
		receivedMessages = nil
		messagesMutex.Unlock()

		// åˆ›å»ºæ–°çš„æ¥æ”¶å™¨ç”¨äºåŠ å¯†æ¶ˆæ¯æµ‹è¯•
		encryptedReceiver := &PullReceiver{
			Config: &Config{
				Option: Option{
					Exchange: "test.exchange",
					Queue:    "test.queue.encrypted",
					Router:   "test.encrypted.routing.key",
					SigTyp:   1,                                  // å¯ç”¨AESåŠ å¯†
					SigKey:   "12345678901234567890123456789012", // 32å­—èŠ‚AES-256å¯†é’¥
					Durable:  false,                              // ä½¿ç”¨éæŒä¹…åŒ–ä»¥é¿å…å‚æ•°å†²çª
				},
				IsNack: false,
			},
			Callback: func(msg *MsgData) error {
				// è§£ææ¶ˆæ¯å†…å®¹
				var content map[string]interface{}
				if err := json.Unmarshal([]byte(msg.Content), &content); err != nil {
					t.Errorf("Failed to parse encrypted message content: %v", err)
					return err
				}

				messagesMutex.Lock()
				receivedMessages = append(receivedMessages, content)
				messagesMutex.Unlock()

				t.Logf("Received encrypted message: %v", content)
				return nil
			},
		}

		// åˆå§‹åŒ–åŠ å¯†æ¥æ”¶å™¨
		encryptedReceiver.initDefaults()
		encryptedReceiver.initControlChans()

		// æ·»åŠ åŠ å¯†æ¥æ”¶å™¨åˆ°ç®¡ç†å™¨
		err = mgr.AddPullReceiver(encryptedReceiver)
		if err != nil {
			t.Fatalf("Failed to add encrypted receiver: %v", err)
		}

		// å¯åŠ¨goroutineå‘é€åŠ å¯†æ¶ˆæ¯
		wg.Add(1)
		go func() {
			defer wg.Done()

			// ç­‰å¾…æ¶ˆè´¹è€…å¯åŠ¨
			time.Sleep(2 * time.Second)

			// åˆ›å»ºå‘å¸ƒç®¡ç†å™¨å‘é€åŠ å¯†æ¶ˆæ¯
			pubMgr, err := NewPublishManager(conf)
			if err != nil {
				t.Errorf("Failed to create publish manager: %v", err)
				return
			}
			defer pubMgr.Close()

			encryptedContent := map[string]interface{}{
				"id":          fmt.Sprintf("encrypted-pull-%d", time.Now().Unix()),
				"secret_data": "This is sensitive information that should be encrypted",
				"type":        "encrypted",
				"timestamp":   time.Now().Unix(),
			}
			encryptedContentBytes, _ := json.Marshal(encryptedContent)
			encryptedMsg := &MsgData{
				Content: string(encryptedContentBytes),
				Option: Option{
					Exchange: "test.exchange",
					Queue:    "test.queue.encrypted",
					Router:   "test.encrypted.routing.key",
					SigTyp:   1,                                  // å¯ç”¨AESåŠ å¯†
					SigKey:   "12345678901234567890123456789012", // 32å­—èŠ‚AES-256å¯†é’¥
					Durable:  true,
				},
				Type: 1,
			}

			err = pubMgr.PublishMsgData(ctx, encryptedMsg)
			if err != nil {
				t.Errorf("Failed to publish encrypted message: %v", err)
				return
			}

			t.Logf("Successfully published encrypted message")
		}()

		// ç­‰å¾…åŠ å¯†æ¶ˆæ¯è¢«æ¶ˆè´¹
		timeout := time.After(10 * time.Second)
		ticker := time.NewTicker(500 * time.Millisecond)
		defer ticker.Stop()

		for {
			select {
			case <-timeout:
				t.Error("Timeout waiting for encrypted message consumption")
				return
			case <-ticker.C:
				messagesMutex.Lock()
				if len(receivedMessages) > 0 {
					messagesMutex.Unlock()
					goto encryptedReceived
				}
				messagesMutex.Unlock()
			}
		}

	encryptedReceived:
		// éªŒè¯æ¥æ”¶åˆ°çš„åŠ å¯†æ¶ˆæ¯
		messagesMutex.Lock()
		if len(receivedMessages) == 0 {
			t.Error("No encrypted messages were received")
		} else {
			msg := receivedMessages[0]
			assert.Equal(t, "encrypted", msg["type"])
			assert.Contains(t, msg, "secret_data")
			assert.Equal(t, "This is sensitive information that should be encrypted", msg["secret_data"])
			t.Logf("Successfully consumed and decrypted message: %v", msg)
		}
		messagesMutex.Unlock()
	})

	t.Run("HealthCheck", func(t *testing.T) {
		// æµ‹è¯•å¥åº·æ£€æŸ¥
		err := mgr.HealthCheck()
		if err != nil {
			t.Errorf("Health check failed: %v", err)
		} else {
			t.Log("Health check passed")
		}
	})

	// ç­‰å¾…æ‰€æœ‰goroutineå®Œæˆ
	wg.Wait()

	t.Log("Real environment pull test completed successfully")
}

// TestPullReconnectionMechanism æµ‹è¯•æ¶ˆè´¹ç«¯çš„æ–­çº¿é‡è¿æœºåˆ¶
func TestPullReconnectionMechanism(t *testing.T) {
	t.Skip("è·³è¿‡é›†æˆæµ‹è¯•ï¼Œå› ä¸ºéœ€è¦çœŸå®çš„RabbitMQç¯å¢ƒä¸”æµ‹è¯•ç¯å¢ƒå¤æ‚")
	return
	// åŠ è½½RabbitMQé…ç½®æ–‡ä»¶
	configData, err := ioutil.ReadFile("../resource/rabbitmq.json")
	if err != nil {
		t.Fatalf("Failed to read RabbitMQ config file: %v", err)
	}

	var conf AmqpConfig
	if err := json.Unmarshal(configData, &conf); err != nil {
		t.Fatalf("Failed to parse RabbitMQ config: %v", err)
	}

	// åˆå§‹åŒ–æ¶ˆè´¹ç®¡ç†å™¨
	mgr := &PullManager{}
	if err := mgr.InitConfig(conf); err != nil {
		t.Fatalf("Failed to init pull manager: %v", err)
	}
	defer func() {
		if err := mgr.Close(); err != nil {
			t.Logf("Error closing manager: %v", err)
		}
	}()

	// ç”¨äºæ”¶é›†æ¥æ”¶åˆ°çš„æ¶ˆæ¯
	var receivedMessages []map[string]interface{}
	var messagesMutex sync.Mutex

	// åˆ›å»ºæµ‹è¯•æ¥æ”¶å™¨
	receiver := &PullReceiver{
		Config: &Config{
			Option: Option{
				Exchange: "test.reconnect.exchange",
				Queue:    "test.reconnect.queue",
				Router:   "test.reconnect.key",
				SigKey:   "rabbitmq_secret_key_32_bytes_1234567890",
				Durable:  true, // ä¸å‘å¸ƒè€…ä¿æŒä¸€è‡´
			},
			IsNack: false,
		},
		Callback: func(msg *MsgData) error {
			// è§£ææ¶ˆæ¯å†…å®¹
			var content map[string]interface{}
			if err := json.Unmarshal([]byte(msg.Content), &content); err != nil {
				t.Errorf("Failed to parse message content: %v", err)
				return err
			}

			messagesMutex.Lock()
			receivedMessages = append(receivedMessages, content)
			messagesMutex.Unlock()

			t.Logf("Received message: %v", content)
			return nil
		},
	}

	// åˆå§‹åŒ–æ¥æ”¶å™¨
	receiver.initDefaults()
	receiver.initControlChans()

	// æ·»åŠ æ¥æ”¶å™¨åˆ°ç®¡ç†å™¨
	err = mgr.AddPullReceiver(receiver)
	if err != nil {
		t.Fatalf("Failed to add receiver: %v", err)
	}

	ctx := context.Background()

	// ç¬¬ä¸€é˜¶æ®µï¼šéªŒè¯åˆå§‹è¿æ¥æ­£å¸¸å·¥ä½œ
	t.Run("InitialConnection", func(t *testing.T) {
		// æ¸…ç©ºä¹‹å‰çš„æ¶ˆæ¯
		messagesMutex.Lock()
		receivedMessages = nil
		messagesMutex.Unlock()

		// ç­‰å¾…æ¶ˆè´¹è€…å¯åŠ¨
		time.Sleep(2 * time.Second)

		// åˆ›å»ºå‘å¸ƒç®¡ç†å™¨å‘é€æµ‹è¯•æ¶ˆæ¯
		pubMgr, err := NewPublishManager(conf)
		if err != nil {
			t.Errorf("Failed to create publish manager: %v", err)
			return
		}
		defer pubMgr.Close()

		testData := map[string]interface{}{
			"id":        "reconnect-test-1",
			"message":   "Message before disconnection",
			"timestamp": time.Now().Unix(),
			"phase":     "before_disconnect",
		}
		testDataBytes, _ := json.Marshal(testData)

		err = pubMgr.Publish(ctx, "test.reconnect.exchange", "test.reconnect.queue", 1, string(testDataBytes), WithDurable(true))
		if err != nil {
			t.Errorf("Failed to publish test message: %v", err)
			return
		}

		t.Logf("Successfully published message before disconnect")

		// ç­‰å¾…æ¶ˆæ¯è¢«æ¶ˆè´¹
		timeout := time.After(10 * time.Second)
		for {
			select {
			case <-timeout:
				t.Error("Timeout waiting for initial message")
				return
			default:
				messagesMutex.Lock()
				if len(receivedMessages) > 0 {
					messagesMutex.Unlock()
					goto initialMessageReceived
				}
				messagesMutex.Unlock()
				time.Sleep(500 * time.Millisecond)
			}
		}

	initialMessageReceived:
		messagesMutex.Lock()
		if len(receivedMessages) > 0 {
			msg := receivedMessages[0]
			assert.Equal(t, "before_disconnect", msg["phase"])
			t.Logf("Successfully consumed initial message: %v", msg)
		}
		messagesMutex.Unlock()
	})

	// ç¬¬äºŒé˜¶æ®µï¼šæ¨¡æ‹Ÿè¿æ¥æ–­å¼€
	t.Run("SimulateDisconnection", func(t *testing.T) {
		t.Log("Simulating connection disconnection...")

		// å¼ºåˆ¶æ–­å¼€è¿æ¥
		mgr.mu.Lock()
		if mgr.conn != nil {
			originalConn := mgr.conn
			mgr.conn.Close()
			mgr.conn = nil
			t.Logf("Connection forcibly closed: %p", originalConn)
		}
		mgr.mu.Unlock()

		// ç­‰å¾…é‡è¿æœºåˆ¶å¯åŠ¨ï¼ˆmonitorConnection åº”è¯¥æ£€æµ‹åˆ°æ–­å¼€å¹¶è§¦å‘é‡è¿ï¼‰
		t.Log("Waiting for reconnection mechanism to activate...")
		time.Sleep(3 * time.Second)

		// éªŒè¯è¿æ¥æ˜¯å¦è¢«æ ‡è®°ä¸ºæ–­å¼€
		mgr.mu.RLock()
		connStatus := mgr.conn
		mgr.mu.RUnlock()

		if connStatus == nil {
			t.Log("Connection successfully marked as disconnected")
		} else {
			t.Log("Connection still exists, reconnection may be in progress")
		}
	})

	// ç¬¬ä¸‰é˜¶æ®µï¼šéªŒè¯é‡è¿æœºåˆ¶è§¦å‘
	t.Run("VerifyReconnectionTrigger", func(t *testing.T) {
		// ç­‰å¾…ä¸€æ®µæ—¶é—´è®©é‡è¿æœºåˆ¶æœ‰æœºä¼šå¯åŠ¨
		time.Sleep(5 * time.Second)

		// æ£€æŸ¥è¿æ¥çŠ¶æ€ - é‡è¿å¯èƒ½éœ€è¦æ›´é•¿æ—¶é—´ï¼Œè¿™é‡Œæˆ‘ä»¬ä¸»è¦éªŒè¯æœºåˆ¶è¢«è§¦å‘
		mgr.mu.RLock()
		conn := mgr.conn
		mgr.mu.RUnlock()

		// è¿æ¥åº”è¯¥ä¸ºnilï¼ˆå·²æ–­å¼€ï¼‰æˆ–è€…æ­£åœ¨é‡è¿è¿‡ç¨‹ä¸­
		if conn == nil {
			t.Log("Connection is nil as expected after disconnection")
		} else if conn.IsClosed() {
			t.Log("Connection is closed, reconnection should be in progress")
		} else {
			t.Log("Connection still exists, reconnection may have completed")
		}

		// éªŒè¯é‡è¿æœºåˆ¶è‡³å°‘è¢«è§¦å‘äº†ï¼ˆé€šè¿‡æ—¥å¿—æˆ‘ä»¬å¯ä»¥çœ‹åˆ°"receiver reconnecting"ï¼‰
		t.Log("Reconnection mechanism verification completed")
	})

	// ç¬¬å››é˜¶æ®µï¼šæµ‹è¯•å¥åº·æ£€æŸ¥
	t.Run("HealthCheckAfterReconnection", func(t *testing.T) {
		err := mgr.HealthCheck()
		if err != nil {
			t.Errorf("Health check failed after reconnection: %v", err)
		} else {
			t.Log("Health check passed after reconnection")
		}
	})

	t.Log("Pull reconnection mechanism test completed successfully")
}

// TestEndToEndReconnectionScenario ç«¯åˆ°ç«¯é‡è¿åœºæ™¯æµ‹è¯•
// é‡ç‚¹éªŒè¯é‡è¿åå‘å¸ƒå’Œæ¶ˆè´¹çš„å®Œæ•´æ¶ˆæ¯æµæ˜¯å¦æ­£å¸¸
func TestEndToEndReconnectionScenario(t *testing.T) {
	t.Skip("è·³è¿‡å®Œæ•´çš„ç«¯åˆ°ç«¯æµ‹è¯•ï¼Œæ”¹ä¸ºè¿è¡Œç®€åŒ–çš„é‡è¿æ¶ˆæ¯æµæµ‹è¯•")
	return
	// åŠ è½½RabbitMQé…ç½®æ–‡ä»¶
	configData, err := ioutil.ReadFile("../resource/rabbitmq.json")
	if err != nil {
		t.Fatalf("Failed to read RabbitMQ config file: %v", err)
	}

	var conf AmqpConfig
	if err := json.Unmarshal(configData, &conf); err != nil {
		t.Fatalf("Failed to parse RabbitMQ config: %v", err)
	}

	// ä½¿ç”¨ä¸“é—¨çš„æµ‹è¯•é˜Ÿåˆ—ï¼Œé¿å…ä¸å…¶ä»–æµ‹è¯•å†²çª
	testExchange := "test.e2e.reconnect"
	testQueue := "test.e2e.reconnect.queue"
	testRouter := "test.e2e.reconnect.key"
	testDurable := true // Publish é»˜è®¤æ˜¯æŒä¹…åŒ–çš„

	// åˆå§‹åŒ–æ¶ˆè´¹ç®¡ç†å™¨
	pullMgr := &PullManager{}
	if err := pullMgr.InitConfig(conf); err != nil {
		t.Fatalf("Failed to init pull manager: %v", err)
	}
	defer func() {
		if err := pullMgr.Close(); err != nil {
			t.Logf("Error closing pull manager: %v", err)
		}
	}()

	// åˆå§‹åŒ–å‘å¸ƒç®¡ç†å™¨
	pubMgr, err := NewPublishManager(conf)
	if err != nil {
		t.Fatalf("Failed to create publish manager: %v", err)
	}
	defer func() {
		if err := pubMgr.Close(); err != nil {
			t.Logf("Error closing publish manager: %v", err)
		}
	}()

	// ç”¨äºæ”¶é›†æ¥æ”¶åˆ°çš„æ¶ˆæ¯
	var receivedMessages []map[string]interface{}
	var messagesMutex sync.Mutex
	var messageCount int32

	// åˆ›å»ºæµ‹è¯•æ¥æ”¶å™¨
	receiver := &PullReceiver{
		Config: &Config{
			Option: Option{
				Exchange: testExchange,
				Queue:    testQueue,
				Router:   testRouter,
				SigKey:   "rabbitmq_secret_key_32_bytes_1234567890",
				Durable:  false,
			},
			IsNack: false,
		},
		Callback: func(msg *MsgData) error {
			// è§£ææ¶ˆæ¯å†…å®¹
			var content map[string]interface{}
			if err := json.Unmarshal([]byte(msg.Content), &content); err != nil {
				t.Errorf("Failed to parse message content: %v", err)
				return err
			}

			messagesMutex.Lock()
			receivedMessages = append(receivedMessages, content)
			atomic.AddInt32(&messageCount, 1)
			messagesMutex.Unlock()

			t.Logf("Received message: phase=%v, id=%v, count=%d",
				content["phase"], content["id"], atomic.LoadInt32(&messageCount))
			return nil
		},
	}

	// åˆå§‹åŒ–æ¥æ”¶å™¨
	receiver.initDefaults()
	receiver.initControlChans()

	// æ·»åŠ æ¥æ”¶å™¨åˆ°ç®¡ç†å™¨
	err = pullMgr.AddPullReceiver(receiver)
	if err != nil {
		t.Fatalf("Failed to add receiver: %v", err)
	}

	ctx := context.Background()

	// ç¬¬ä¸€é˜¶æ®µï¼šé‡è¿å‰çš„åŸºç¡€æ¶ˆæ¯æµæµ‹è¯•
	t.Run("PreReconnectionMessageFlow", func(t *testing.T) {
		// å‘é€3æ¡é‡è¿å‰çš„æ¶ˆæ¯
		for i := 0; i < 3; i++ {
			testData := map[string]interface{}{
				"id":        fmt.Sprintf("pre-reconnect-%d", i+1),
				"message":   fmt.Sprintf("Message before reconnection %d", i+1),
				"timestamp": time.Now().Unix(),
				"phase":     "pre_reconnect",
				"sequence":  i + 1,
			}
			testDataBytes, _ := json.Marshal(testData)

			err := pubMgr.Publish(ctx, testExchange, testQueue, 1, string(testDataBytes), WithDurable(testDurable))
			if err != nil {
				t.Errorf("Failed to publish pre-reconnect message %d: %v", i+1, err)
				continue
			}
			t.Logf("Published pre-reconnect message %d", i+1)

			// å°å»¶è¿Ÿç¡®ä¿æ¶ˆæ¯é¡ºåº
			time.Sleep(100 * time.Millisecond)
		}

		// ç­‰å¾…æ‰€æœ‰æ¶ˆæ¯è¢«æ¶ˆè´¹
		timeout := time.After(10 * time.Second)
		for {
			select {
			case <-timeout:
				t.Fatalf("Timeout waiting for pre-reconnect messages. Received: %d, Expected: 3",
					atomic.LoadInt32(&messageCount))
			default:
				if atomic.LoadInt32(&messageCount) >= 3 {
					goto preReconnectComplete
				}
				time.Sleep(200 * time.Millisecond)
			}
		}

	preReconnectComplete:
		messagesMutex.Lock()
		if len(receivedMessages) != 3 {
			t.Errorf("Expected 3 messages, got %d", len(receivedMessages))
		}
		for _, msg := range receivedMessages {
			if msg["phase"] != "pre_reconnect" {
				t.Errorf("Expected phase 'pre_reconnect', got %v", msg["phase"])
			}
		}
		messagesMutex.Unlock()

		t.Logf("Pre-reconnection message flow test passed: %d messages processed",
			atomic.LoadInt32(&messageCount))
	})

	// ç¬¬äºŒé˜¶æ®µï¼šåŒæ—¶æ–­å¼€å‘å¸ƒå’Œæ¶ˆè´¹è¿æ¥
	t.Run("SimultaneousConnectionDisruption", func(t *testing.T) {
		t.Log("Simulating simultaneous connection disruption...")

		// æ–­å¼€æ¶ˆè´¹è¿æ¥
		pullMgr.mu.Lock()
		if pullMgr.conn != nil {
			if err := pullMgr.conn.Close(); err != nil {
				t.Logf("Pull connection close error: %v", err)
			}
			pullMgr.conn = nil
		}
		pullMgr.mu.Unlock()

		// æ–­å¼€å‘å¸ƒè¿æ¥
		pubMgr.mu.Lock()
		if pubMgr.conn != nil {
			if err := pubMgr.conn.Close(); err != nil {
				t.Logf("Publish connection close error: %v", err)
			}
			pubMgr.conn = nil
		}
		pubMgr.mu.Unlock()

		t.Log("Both connections forcibly closed")

		// ç­‰å¾…é‡è¿æœºåˆ¶å¯åŠ¨
		time.Sleep(3 * time.Second)
	})

	// ç¬¬ä¸‰é˜¶æ®µï¼šé‡è¿åçš„æ¶ˆæ¯æµæµ‹è¯•
	t.Run("PostReconnectionMessageFlow", func(t *testing.T) {
		// ç­‰å¾…æ›´é•¿æ—¶é—´ç¡®ä¿é‡è¿å®Œæˆ
		t.Log("Waiting for both publish and pull reconnections to complete...")
		maxWaitTime := 45 * time.Second
		reconnectTimeout := time.After(maxWaitTime)
		reconnectStart := time.Now()

		// å®šæœŸæ£€æŸ¥è¿æ¥çŠ¶æ€
		ticker := time.NewTicker(3 * time.Second)
		defer ticker.Stop()

		connectionsReady := false
		for !connectionsReady {
			select {
			case <-reconnectTimeout:
				t.Fatalf("Reconnection did not complete within %v", maxWaitTime)
			case <-ticker.C:
				// æ£€æŸ¥å‘å¸ƒè¿æ¥
				pubMgr.mu.RLock()
				pubConnReady := pubMgr.conn != nil && !pubMgr.conn.IsClosed()
				pubMgr.mu.RUnlock()

				// æ£€æŸ¥æ¶ˆè´¹è¿æ¥
				pullMgr.mu.RLock()
				pullConnReady := pullMgr.conn != nil && !pullMgr.conn.IsClosed()
				pullMgr.mu.RUnlock()

				if pubConnReady && pullConnReady {
					elapsed := time.Since(reconnectStart)
					t.Logf("Both connections restored after %v", elapsed)
					connectionsReady = true
				} else {
					t.Logf("Waiting for connections... Publish: %v, Pull: %v",
						pubConnReady, pullConnReady)
				}
			}
		}

		// è¿æ¥æ¢å¤åï¼Œå‘é€é‡è¿åçš„æµ‹è¯•æ¶ˆæ¯
		postReconnectStart := atomic.LoadInt32(&messageCount)

		// å‘é€5æ¡é‡è¿åçš„æ¶ˆæ¯
		for i := 0; i < 5; i++ {
			testData := map[string]interface{}{
				"id":        fmt.Sprintf("post-reconnect-%d", i+1),
				"message":   fmt.Sprintf("Message after reconnection %d", i+1),
				"timestamp": time.Now().Unix(),
				"phase":     "post_reconnect",
				"sequence":  i + 1,
			}
			testDataBytes, _ := json.Marshal(testData)

			// ä½¿ç”¨ä¸åŒçš„è·¯ç”±é”®æµ‹è¯•é€šé“é‡å»º
			router := fmt.Sprintf("%s.%d", testRouter, i+1)
			err := pubMgr.Publish(ctx, testExchange, testQueue, 1, string(testDataBytes),
				WithRouter(router), WithDurable(testDurable))
			if err != nil {
				t.Errorf("Failed to publish post-reconnect message %d: %v", i+1, err)
				continue
			}
			t.Logf("Published post-reconnect message %d", i+1)

			// å°å»¶è¿Ÿç¡®ä¿æ¶ˆæ¯é¡ºåº
			time.Sleep(200 * time.Millisecond)
		}

		// ç­‰å¾…æ‰€æœ‰é‡è¿åçš„æ¶ˆæ¯è¢«æ¶ˆè´¹
		messageTimeout := time.After(20 * time.Second)
		expectedTotal := postReconnectStart + 5

		for {
			select {
			case <-messageTimeout:
				currentCount := atomic.LoadInt32(&messageCount)
				t.Fatalf("Timeout waiting for post-reconnect messages. Received: %d, Expected: %d",
					currentCount, expectedTotal)
			default:
				if atomic.LoadInt32(&messageCount) >= expectedTotal {
					goto postReconnectComplete
				}
				time.Sleep(300 * time.Millisecond)
			}
		}

	postReconnectComplete:
		// éªŒè¯é‡è¿åçš„æ¶ˆæ¯
		messagesMutex.Lock()
		postReconnectMessages := 0
		for _, msg := range receivedMessages {
			if msg["phase"] == "post_reconnect" {
				postReconnectMessages++
			}
		}

		if postReconnectMessages != 5 {
			t.Errorf("Expected 5 post-reconnect messages, got %d", postReconnectMessages)
		}

		// éªŒè¯æ¶ˆæ¯é¡ºåºå’Œå®Œæ•´æ€§
		postMessages := make([]map[string]interface{}, 0)
		for _, msg := range receivedMessages {
			if msg["phase"] == "post_reconnect" {
				postMessages = append(postMessages, msg)
			}
		}

		// æŒ‰sequenceæ’åºéªŒè¯
		for i, msg := range postMessages {
			expectedSeq := i + 1
			if int(msg["sequence"].(float64)) != expectedSeq {
				t.Errorf("Message sequence mismatch at index %d: expected %d, got %v",
					i, expectedSeq, msg["sequence"])
			}
		}

		messagesMutex.Unlock()

		t.Logf("Post-reconnection message flow test passed: %d total messages processed, %d post-reconnect messages",
			atomic.LoadInt32(&messageCount), postReconnectMessages)
	})

	// ç¬¬å››é˜¶æ®µï¼šæ‰¹é‡å‘å¸ƒæµ‹è¯•
	t.Run("BatchPublishAfterReconnection", func(t *testing.T) {
		batchSize := 3
		msgs := make([]*MsgData, batchSize)

		batchStartCount := atomic.LoadInt32(&messageCount)

		for i := 0; i < batchSize; i++ {
			contentData := map[string]interface{}{
				"id":          fmt.Sprintf("batch-post-reconnect-%d", i+1),
				"message":     fmt.Sprintf("Batch message after reconnection %d", i+1),
				"type":        "batch_reconnect_test",
				"batch_index": i + 1,
				"timestamp":   time.Now().Unix(),
				"phase":       "batch_post_reconnect",
			}
			contentBytes, _ := json.Marshal(contentData)
			msgs[i] = &MsgData{
				Content: string(contentBytes),
				Option: Option{
					Exchange: testExchange,
					Queue:    testQueue,
					Router:   testRouter,
					Durable:  false,
				},
				Type: 1,
			}
		}

		err := pubMgr.BatchPublish(ctx, msgs)
		if err != nil {
			t.Errorf("Batch publish after reconnection failed: %v", err)
			return
		}

		t.Logf("Successfully published %d batch messages after reconnection", batchSize)

		// ç­‰å¾…æ‰¹é‡æ¶ˆæ¯è¢«æ¶ˆè´¹
		batchTimeout := time.After(15 * time.Second)
		expectedAfterBatch := batchStartCount + int32(batchSize)

		for {
			select {
			case <-batchTimeout:
				currentCount := atomic.LoadInt32(&messageCount)
				t.Fatalf("Timeout waiting for batch messages. Received: %d, Expected: %d",
					currentCount, expectedAfterBatch)
			default:
				if atomic.LoadInt32(&messageCount) >= expectedAfterBatch {
					goto batchComplete
				}
				time.Sleep(200 * time.Millisecond)
			}
		}

	batchComplete:
		// éªŒè¯æ‰¹é‡æ¶ˆæ¯
		messagesMutex.Lock()
		batchMessages := 0
		for _, msg := range receivedMessages {
			if msg["phase"] == "batch_post_reconnect" {
				batchMessages++
			}
		}

		if batchMessages != batchSize {
			t.Errorf("Expected %d batch messages, got %d", batchSize, batchMessages)
		}
		messagesMutex.Unlock()

		t.Logf("Batch publish test passed: %d batch messages processed", batchMessages)
	})

	// ç¬¬äº”é˜¶æ®µï¼šæœ€ç»ˆå¥åº·æ£€æŸ¥
	t.Run("FinalHealthCheck", func(t *testing.T) {
		// æ£€æŸ¥å‘å¸ƒç®¡ç†å™¨å¥åº·çŠ¶æ€
		pubHealthy := pubMgr.HealthCheck()
		if pubHealthy != nil {
			t.Errorf("Publish manager health check failed after full reconnection test: %v", pubHealthy)
		}

		// æ£€æŸ¥æ¶ˆè´¹ç®¡ç†å™¨å¥åº·çŠ¶æ€
		pullHealthy := pullMgr.HealthCheck()
		if pullHealthy != nil {
			t.Errorf("Pull manager health check failed after full reconnection test: %v", pullHealthy)
		}

		// æ£€æŸ¥æ¥æ”¶å™¨å¥åº·çŠ¶æ€
		if !receiver.IsHealthy() {
			t.Error("Receiver is not healthy after reconnection test")
		}

		// æœ€ç»ˆç»Ÿè®¡
		finalCount := atomic.LoadInt32(&messageCount)
		messagesMutex.Lock()
		totalReceived := len(receivedMessages)
		messagesMutex.Unlock()

		t.Logf("Final health check passed - Total messages processed: %d, Messages in buffer: %d",
			finalCount, totalReceived)

		// éªŒè¯æ¶ˆæ¯å®Œæ•´æ€§
		expectedTotal := 3 + 5 + 3 // pre + post + batch
		if finalCount != int32(expectedTotal) {
			t.Errorf("Message count mismatch: expected %d, got %d", expectedTotal, finalCount)
		}
	})

	t.Logf("End-to-end reconnection scenario test completed successfully! Total messages: %d",
		atomic.LoadInt32(&messageCount))
}

// TestReconnectionMessageFlow é‡ç‚¹æµ‹è¯•é‡è¿åæ¶ˆæ¯å‘é€å’Œæ¥æ”¶åŠŸèƒ½
func TestReconnectionMessageFlow(t *testing.T) {
	// åŠ è½½RabbitMQé…ç½®æ–‡ä»¶
	configData, err := ioutil.ReadFile("../resource/rabbitmq.json")
	if err != nil {
		t.Fatalf("Failed to read RabbitMQ config file: %v", err)
	}

	var conf AmqpConfig
	if err := json.Unmarshal(configData, &conf); err != nil {
		t.Fatalf("Failed to parse RabbitMQ config: %v", err)
	}

	// åˆå§‹åŒ–æ¶ˆè´¹ç®¡ç†å™¨
	pullMgr := &PullManager{}
	if err := pullMgr.InitConfig(conf); err != nil {
		t.Fatalf("Failed to init pull manager: %v", err)
	}
	defer func() {
		if err := pullMgr.Close(); err != nil {
			t.Logf("Error closing pull manager: %v", err)
		}
	}()

	// åˆå§‹åŒ–å‘å¸ƒç®¡ç†å™¨
	pubMgr, err := NewPublishManager(conf)
	if err != nil {
		t.Fatalf("Failed to create publish manager: %v", err)
	}
	defer func() {
		if err := pubMgr.Close(); err != nil {
			t.Logf("Error closing publish manager: %v", err)
		}
	}()

	// ç”¨äºæ”¶é›†æ¥æ”¶åˆ°çš„æ¶ˆæ¯
	var receivedMessages []map[string]interface{}
	var messagesMutex sync.Mutex
	var messageCount int32

	// åˆ›å»ºæµ‹è¯•æ¥æ”¶å™¨
	receiver := &PullReceiver{
		Config: &Config{
			Option: Option{
				Exchange: "test.flow.reconnect",
				Queue:    "test.flow.reconnect.queue",
				Router:   "test.flow.reconnect.key",
				SigKey:   "rabbitmq_secret_key_32_bytes_1234567890",
				Durable:  true, // ä¸å‘å¸ƒè€…ä¿æŒä¸€è‡´
			},
			IsNack: false,
		},
		Callback: func(msg *MsgData) error {
			// è§£ææ¶ˆæ¯å†…å®¹
			var content map[string]interface{}
			if err := json.Unmarshal([]byte(msg.Content), &content); err != nil {
				t.Errorf("Failed to parse message content: %v", err)
				return err
			}

			messagesMutex.Lock()
			receivedMessages = append(receivedMessages, content)
			atomic.AddInt32(&messageCount, 1)
			messagesMutex.Unlock()

			t.Logf("Received message: id=%v, phase=%v, count=%d",
				content["id"], content["phase"], atomic.LoadInt32(&messageCount))
			return nil
		},
	}

	// åˆå§‹åŒ–æ¥æ”¶å™¨
	receiver.initDefaults()
	receiver.initControlChans()

	// æ·»åŠ æ¥æ”¶å™¨åˆ°ç®¡ç†å™¨
	err = pullMgr.AddPullReceiver(receiver)
	if err != nil {
		t.Fatalf("Failed to add receiver: %v", err)
	}

	ctx := context.Background()

	// ç¬¬ä¸€é˜¶æ®µï¼šéªŒè¯åˆå§‹è¿æ¥æ­£å¸¸å·¥ä½œ
	t.Run("InitialConnectionAndMessageFlow", func(t *testing.T) {
		// å‘é€ä¸€æ¡åˆå§‹æ¶ˆæ¯
		testData := map[string]interface{}{
			"id":        "initial-test",
			"message":   "Initial message before any reconnection",
			"timestamp": time.Now().Unix(),
			"phase":     "initial",
		}
		testDataBytes, _ := json.Marshal(testData)

		err := pubMgr.Publish(ctx, "test.flow.reconnect", "test.flow.reconnect.queue", 1, string(testDataBytes), WithDurable(true))
		if err != nil {
			// å¦‚æœå‘å¸ƒå¤±è´¥ï¼Œå¯èƒ½æ˜¯äº¤æ¢æœºå·²å­˜åœ¨ä½†å‚æ•°ä¸åŒï¼Œæˆ‘ä»¬è·³è¿‡è¿™ä¸ªæµ‹è¯•
			t.Skipf("Initial publish failed (likely due to existing exchange): %v", err)
			return
		}

		// ç­‰å¾…æ¶ˆæ¯è¢«æ¶ˆè´¹
		timeout := time.After(10 * time.Second)
		for {
			select {
			case <-timeout:
				t.Fatalf("Timeout waiting for initial message")
			default:
				if atomic.LoadInt32(&messageCount) >= 1 {
					goto initialComplete
				}
				time.Sleep(200 * time.Millisecond)
			}
		}

	initialComplete:
		messagesMutex.Lock()
		if len(receivedMessages) == 0 || receivedMessages[0]["phase"] != "initial" {
			t.Error("Initial message not received correctly")
		}
		messagesMutex.Unlock()

		t.Log("Initial connection and message flow test passed")
	})

	// ç¬¬äºŒé˜¶æ®µï¼šæ¨¡æ‹Ÿè¿æ¥æ–­å¼€å¹¶ç›´æ¥è§¦å‘é‡è¿
	t.Run("SimulateConnectionDisruption", func(t *testing.T) {
		t.Log("Simulating connection disruption...")

		// æ–­å¼€ä¸¤ä¸ªè¿æ¥
		pullMgr.mu.Lock()
		if pullMgr.conn != nil {
			pullMgr.conn.Close()
			pullMgr.conn = nil
		}
		pullMgr.mu.Unlock()

		pubMgr.mu.Lock()
		if pubMgr.conn != nil {
			pubMgr.conn.Close()
			pubMgr.conn = nil
		}
		pubMgr.mu.Unlock()

		t.Log("Both connections forcibly closed")

		// æ‰‹åŠ¨è§¦å‘é‡è¿ï¼ˆå› ä¸ºå¼‚æ­¥çš„NotifyCloseå¯èƒ½ä¸å·¥ä½œï¼‰
		t.Log("Manually triggering reconnection...")

		// ä¸ºpull managerè§¦å‘é‡è¿
		go func() {
			pullMgr.reconnectAllReceivers()
		}()

		// ä¸ºpublish managerè§¦å‘é‡è¿ï¼ˆé€šè¿‡Connectæ–¹æ³•ï¼‰
		go func() {
			pubMgr.Connect()
			// è¿æ¥é‡å»ºåï¼Œæ‰‹åŠ¨é‡å»ºé€šé“
			time.Sleep(500 * time.Millisecond) // ç­‰å¾…è¿æ¥å»ºç«‹
			pubMgr.rebuildChannels()
		}()

		// ç­‰å¾…é‡è¿å¼€å§‹
		time.Sleep(2 * time.Second)
	})

	// ç¬¬ä¸‰é˜¶æ®µï¼šé‡è¿åéªŒè¯æ¶ˆæ¯æµ
	t.Run("MessageFlowAfterReconnection", func(t *testing.T) {
		// ç­‰å¾…è¿æ¥æ¢å¤ï¼ˆæœ€å¤§ç­‰å¾…30ç§’ï¼‰
		t.Log("Waiting for connections to be restored...")
		maxWaitTime := 30 * time.Second
		reconnectTimeout := time.After(maxWaitTime)
		ticker := time.NewTicker(2 * time.Second)
		defer ticker.Stop()

		connectionsReady := false
		for !connectionsReady {
			select {
			case <-reconnectTimeout:
				t.Fatalf("Connections not restored within %v", maxWaitTime)
			case <-ticker.C:
				pullMgr.mu.RLock()
				pullReady := pullMgr.conn != nil && !pullMgr.conn.IsClosed()
				pullMgr.mu.RUnlock()

				pubMgr.mu.RLock()
				pubReady := pubMgr.conn != nil && !pubMgr.conn.IsClosed()
				pubMgr.mu.RUnlock()

				if pullReady && pubReady {
					connectionsReady = true
					t.Log("Both connections successfully restored!")
				} else {
					t.Logf("Waiting... Pull: %v, Publish: %v", pullReady, pubReady)
				}
			}
		}

		// è¿æ¥æ¢å¤åï¼Œç­‰å¾…é€šé“é‡å»ºï¼Œç„¶åæµ‹è¯•æ¶ˆæ¯å‘é€å’Œæ¥æ”¶
		t.Log("Waiting for channel rebuild after reconnection...")
		time.Sleep(2 * time.Second) // ç»™é€šé“é‡å»ºä¸€äº›æ—¶é—´

		initialCount := atomic.LoadInt32(&messageCount)

		// å‘é€é‡è¿åçš„æ¶ˆæ¯ï¼ˆè¿™åº”è¯¥ä¼šè§¦å‘é€šé“é‡å»ºï¼‰
		testData := map[string]interface{}{
			"id":        "post-reconnect-test",
			"message":   "Message sent after reconnection",
			"timestamp": time.Now().Unix(),
			"phase":     "post_reconnect",
		}
		testDataBytes, _ := json.Marshal(testData)

		// å°è¯•å¤šæ¬¡å‘å¸ƒï¼Œå› ä¸ºé‡è¿åçš„ç¬¬ä¸€æ¬¡å‘å¸ƒå¯èƒ½ä¼šå¤±è´¥
		var lastErr error
		var success bool
		for i := 0; i < 3; i++ {
			err := pubMgr.Publish(ctx, "test.flow.reconnect", "test.flow.reconnect.queue", 1, string(testDataBytes), WithDurable(true))
			if err == nil {
				success = true
				t.Log("Successfully published message after reconnection")
				break
			}
			lastErr = err
			t.Logf("Publish attempt %d failed: %v, retrying...", i+1, err)
			time.Sleep(500 * time.Millisecond)
		}

		if !success {
			t.Errorf("Failed to publish message after reconnection after retries: %v", lastErr)
			return
		}

		// ç­‰å¾…æ¶ˆæ¯è¢«æ¶ˆè´¹
		messageTimeout := time.After(15 * time.Second)
		expectedCount := initialCount + 1

		for {
			select {
			case <-messageTimeout:
				currentCount := atomic.LoadInt32(&messageCount)
				t.Fatalf("Timeout waiting for post-reconnect message. Expected: %d, Got: %d",
					expectedCount, currentCount)
			default:
				if atomic.LoadInt32(&messageCount) >= expectedCount {
					goto messageReceived
				}
				time.Sleep(300 * time.Millisecond)
			}
		}

	messageReceived:
		// éªŒè¯æ¥æ”¶åˆ°çš„æ¶ˆæ¯
		messagesMutex.Lock()
		found := false
		for _, msg := range receivedMessages {
			if msg["phase"] == "post_reconnect" && msg["id"] == "post-reconnect-test" {
				found = true
				break
			}
		}
		messagesMutex.Unlock()

		if !found {
			t.Error("Post-reconnection message not found in received messages")
		}

		t.Logf("Message flow after reconnection test passed! Total messages: %d",
			atomic.LoadInt32(&messageCount))
	})

	// ç¬¬å››é˜¶æ®µï¼šå¥åº·æ£€æŸ¥
	t.Run("HealthCheckAfterReconnection", func(t *testing.T) {
		// æ£€æŸ¥å‘å¸ƒç®¡ç†å™¨
		if err := pubMgr.HealthCheck(); err != nil {
			t.Errorf("Publish manager health check failed: %v", err)
		}

		// æ£€æŸ¥æ¶ˆè´¹ç®¡ç†å™¨
		if err := pullMgr.HealthCheck(); err != nil {
			t.Errorf("Pull manager health check failed: %v", err)
		}

		// æ£€æŸ¥æ¥æ”¶å™¨
		if !receiver.IsHealthy() {
			t.Error("Receiver is not healthy after reconnection")
		}

		t.Log("All health checks passed after reconnection")
	})

	finalCount := atomic.LoadInt32(&messageCount)
	t.Logf("Reconnection message flow test completed successfully! Total messages processed: %d", finalCount)
}

// TestBasicPublishConsumeFlow åŸºæœ¬çš„å‘å¸ƒæ¶ˆè´¹æµç¨‹æ¼”ç¤º
// å±•ç¤ºå•çº¿ç¨‹çš„å‘å¸ƒå’Œæ¶ˆè´¹è¿‡ç¨‹ï¼Œä¸åŒ…å«é‡è¿
func TestBasicPublishConsumeFlow(t *testing.T) {
	// åŠ è½½RabbitMQé…ç½®æ–‡ä»¶
	configData, err := ioutil.ReadFile("../resource/rabbitmq.json")
	if err != nil {
		t.Fatalf("Failed to read RabbitMQ config file: %v", err)
	}

	var conf AmqpConfig
	if err := json.Unmarshal(configData, &conf); err != nil {
		t.Fatalf("Failed to parse RabbitMQ config: %v", err)
	}

	// ä½¿ç”¨ä¸“é—¨çš„æµ‹è¯•èµ„æº
	testExchange := "test.basic.flow"
	testQueue := "test.basic.flow.queue"
	testRouter := "test.basic.flow.key"

	t.Logf("ğŸš€ Starting basic publish/consume flow test")
	t.Logf("   Exchange: %s", testExchange)
	t.Logf("   Queue: %s", testQueue)
	t.Logf("   Router: %s", testRouter)

	// åˆå§‹åŒ–æ¶ˆè´¹ç®¡ç†å™¨
	pullMgr := &PullManager{}
	if err := pullMgr.InitConfig(conf); err != nil {
		t.Fatalf("Failed to init pull manager: %v", err)
	}
	defer func() {
		if err := pullMgr.Close(); err != nil {
			t.Logf("Error closing pull manager: %v", err)
		}
	}()

	// åˆå§‹åŒ–å‘å¸ƒç®¡ç†å™¨
	pubMgr, err := NewPublishManager(conf)
	if err != nil {
		t.Fatalf("Failed to create publish manager: %v", err)
	}
	defer func() {
		if err := pubMgr.Close(); err != nil {
			t.Logf("Error closing publish manager: %v", err)
		}
	}()

	// ç”¨äºæ”¶é›†æ¥æ”¶åˆ°çš„æ¶ˆæ¯
	var receivedMessages []map[string]interface{}
	var messageCount int32

	// åˆ›å»ºæµ‹è¯•æ¥æ”¶å™¨
	receiver := &PullReceiver{
		Config: &Config{
			Option: Option{
				Exchange: testExchange,
				Queue:    testQueue,
				Router:   testRouter,
				SigKey:   "rabbitmq_secret_key_32_bytes_1234567890",
				Durable:  true,
			},
			IsNack: false,
		},
		Callback: func(msg *MsgData) error {
			// è§£ææ¶ˆæ¯å†…å®¹
			var content map[string]interface{}
			if err := json.Unmarshal([]byte(msg.Content), &content); err != nil {
				t.Errorf("Failed to parse message content: %v", err)
				return err
			}

			atomic.AddInt32(&messageCount, 1)
			receivedMessages = append(receivedMessages, content)

			count := atomic.LoadInt32(&messageCount)
			t.Logf("ğŸ“¨ RECEIVED Message #%d:", count)
			t.Logf("   ID: %v", content["id"])
			t.Logf("   Message: %v", content["message"])
			t.Logf("   Phase: %v", content["phase"])
			t.Logf("   Timestamp: %v", content["timestamp"])

			return nil
		},
	}

	// åˆå§‹åŒ–æ¥æ”¶å™¨
	receiver.initDefaults()
	receiver.initControlChans()

	// æ·»åŠ æ¥æ”¶å™¨åˆ°ç®¡ç†å™¨
	err = pullMgr.AddPullReceiver(receiver)
	if err != nil {
		t.Fatalf("Failed to add receiver: %v", err)
	}

	ctx := context.Background()
	time.Sleep(1 * time.Second) // ç­‰å¾…æ¶ˆè´¹è€…å¯åŠ¨

	// ç¬¬ä¸€é˜¶æ®µï¼šå‘å¸ƒç¬¬ä¸€æ¡æ¶ˆæ¯
	t.Run("PublishFirstMessage", func(t *testing.T) {
		t.Log("ğŸ“¤ PUBLISHING first message...")

		testData := map[string]interface{}{
			"id":        "msg-001",
			"message":   "This is the first message in the basic flow",
			"phase":     "first",
			"timestamp": time.Now().Unix(),
		}
		testDataBytes, _ := json.Marshal(testData)

		t.Logf("   Sending: %s", string(testDataBytes))

		err := pubMgr.Publish(ctx, testExchange, testQueue, 1, string(testDataBytes))
		if err != nil {
			t.Fatalf("Failed to publish first message: %v", err)
		}

		t.Log("âœ… First message published successfully")

		// ç­‰å¾…æ¶ˆæ¯è¢«æ¶ˆè´¹
		timeout := time.After(5 * time.Second)
		for {
			select {
			case <-timeout:
				t.Fatalf("Timeout waiting for first message")
			default:
				if atomic.LoadInt32(&messageCount) >= 1 {
					goto firstMessageReceived
				}
				time.Sleep(100 * time.Millisecond)
			}
		}

	firstMessageReceived:
		t.Log("âœ… First message consumed successfully")
	})

	// ç¬¬äºŒé˜¶æ®µï¼šå‘å¸ƒç¬¬äºŒæ¡æ¶ˆæ¯
	t.Run("PublishSecondMessage", func(t *testing.T) {
		t.Log("ğŸ“¤ PUBLISHING second message...")

		testData := map[string]interface{}{
			"id":        "msg-002",
			"message":   "This is the second message in the basic flow",
			"phase":     "second",
			"timestamp": time.Now().Unix(),
		}
		testDataBytes, _ := json.Marshal(testData)

		t.Logf("   Sending: %s", string(testDataBytes))

		err := pubMgr.Publish(ctx, testExchange, testQueue, 1, string(testDataBytes))
		if err != nil {
			t.Fatalf("Failed to publish second message: %v", err)
		}

		t.Log("âœ… Second message published successfully")

		// ç­‰å¾…æ¶ˆæ¯è¢«æ¶ˆè´¹
		timeout := time.After(5 * time.Second)
		for {
			select {
			case <-timeout:
				t.Fatalf("Timeout waiting for second message")
			default:
				if atomic.LoadInt32(&messageCount) >= 2 {
					goto secondMessageReceived
				}
				time.Sleep(100 * time.Millisecond)
			}
		}

	secondMessageReceived:
		t.Log("âœ… Second message consumed successfully")
	})

	// ç¬¬ä¸‰é˜¶æ®µï¼šå‘å¸ƒç¬¬ä¸‰æ¡æ¶ˆæ¯
	t.Run("PublishThirdMessage", func(t *testing.T) {
		t.Log("ğŸ“¤ PUBLISHING third message...")

		testData := map[string]interface{}{
			"id":        "msg-003",
			"message":   "This is the third message in the basic flow",
			"phase":     "third",
			"timestamp": time.Now().Unix(),
		}
		testDataBytes, _ := json.Marshal(testData)

		t.Logf("   Sending: %s", string(testDataBytes))

		err := pubMgr.Publish(ctx, testExchange, testQueue, 1, string(testDataBytes))
		if err != nil {
			t.Fatalf("Failed to publish third message: %v", err)
		}

		t.Log("âœ… Third message published successfully")

		// ç­‰å¾…æ¶ˆæ¯è¢«æ¶ˆè´¹
		timeout := time.After(5 * time.Second)
		for {
			select {
			case <-timeout:
				t.Fatalf("Timeout waiting for third message")
			default:
				if atomic.LoadInt32(&messageCount) >= 3 {
					goto thirdMessageReceived
				}
				time.Sleep(100 * time.Millisecond)
			}
		}

	thirdMessageReceived:
		t.Log("âœ… Third message consumed successfully")
	})

	// ç¬¬å››é˜¶æ®µï¼šæœ€ç»ˆéªŒè¯
	t.Run("FinalVerification", func(t *testing.T) {
		t.Log("ğŸ FINAL VERIFICATION")

		// å¥åº·æ£€æŸ¥
		pubHealthy := pubMgr.HealthCheck()
		pullHealthy := pullMgr.HealthCheck()

		if pubHealthy != nil {
			t.Errorf("âŒ Publish manager health check failed: %v", pubHealthy)
		} else {
			t.Log("âœ… Publish manager health check passed")
		}

		if pullHealthy != nil {
			t.Errorf("âŒ Pull manager health check failed: %v", pullHealthy)
		} else {
			t.Log("âœ… Pull manager health check passed")
		}

		if !receiver.IsHealthy() {
			t.Error("âŒ Receiver is not healthy")
		} else {
			t.Log("âœ… Receiver health check passed")
		}

		// æ¶ˆæ¯ç»Ÿè®¡
		totalMessages := atomic.LoadInt32(&messageCount)
		expectedTotal := 3 // 3ä¸ªå•æ¡æ¶ˆæ¯

		t.Logf("ğŸ“Š MESSAGE STATISTICS:")
		t.Logf("   ğŸ“¨ Total messages received: %d", totalMessages)
		t.Logf("   ğŸ“¤ Expected messages: %d", expectedTotal)

		if totalMessages != int32(expectedTotal) {
			t.Errorf("âŒ Message count mismatch: expected %d, got %d", expectedTotal, totalMessages)
		} else {
			t.Logf("âœ… All %d messages processed correctly", totalMessages)
		}

		// éªŒè¯æ¶ˆæ¯å†…å®¹
		t.Log("ğŸ“‹ MESSAGE DETAILS:")
		for i, msg := range receivedMessages {
			t.Logf("   %d. ID: %v, Phase: %v, Message: %.50s...",
				i+1, msg["id"], msg["phase"], msg["message"])
		}
	})

	finalCount := atomic.LoadInt32(&messageCount)
	t.Logf("ğŸ‰ Basic publish/consume flow test completed successfully!")
	t.Logf("   ğŸ“Š Total messages processed: %d", finalCount)
	t.Logf("   âœ… Publish operations: successful")
	t.Logf("   âœ… Consume operations: successful")
	t.Logf("   âœ… Message integrity: maintained")
	t.Logf("   âœ… System health: good")
}

// TestPublishPullConcurrentOperations å¹¶å‘å‘å¸ƒæ¶ˆè´¹æ“ä½œçš„å®Œæ•´é›†æˆæµ‹è¯•
// æµ‹è¯•åœºæ™¯ï¼špublishå’ŒpullåŒæ—¶å­˜åœ¨ï¼ŒåŒæ—¶åˆ›å»ºæ–°çš„exchangeå’Œqueueï¼Œè§‚å¯ŸæŠ¢å åˆ›å»ºè¡Œä¸ºï¼Œ
// å¹¶å‘å‘é€å’Œæ¶ˆè´¹æ¶ˆæ¯ï¼Œç„¶ååˆ†åˆ«æ–­çº¿é‡è¿éªŒè¯åŠŸèƒ½æ¢å¤
func TestPublishPullConcurrentOperations(t *testing.T) {
	// åŠ è½½RabbitMQé…ç½®æ–‡ä»¶
	configData, err := ioutil.ReadFile("../resource/rabbitmq.json")
	if err != nil {
		t.Fatalf("Failed to read RabbitMQ config file: %v", err)
	}

	var conf AmqpConfig
	if err := json.Unmarshal(configData, &conf); err != nil {
		t.Fatalf("Failed to parse RabbitMQ config: %v", err)
	}

	// ä½¿ç”¨ä¸“é—¨çš„æµ‹è¯•èµ„æºï¼Œé¿å…ä¸å…¶ä»–æµ‹è¯•å†²çª
	testExchange := "test.concurrent.ops"
	testQueue := "test.concurrent.ops.queue"
	testRouter := "test.concurrent.ops.key"

	t.Logf("Starting concurrent publish/pull operations test with exchange: %s, queue: %s",
		testExchange, testQueue)

	// åˆå§‹åŒ–å‘å¸ƒç®¡ç†å™¨
	pubMgr, err := NewPublishManager(conf)
	if err != nil {
		t.Fatalf("Failed to create publish manager: %v", err)
	}
	defer func() {
		if err := pubMgr.Close(); err != nil {
			t.Logf("Error closing publish manager: %v", err)
		}
	}()

	// åˆå§‹åŒ–æ¶ˆè´¹ç®¡ç†å™¨
	pullMgr := &PullManager{}
	if err := pullMgr.InitConfig(conf); err != nil {
		t.Fatalf("Failed to init pull manager: %v", err)
	}
	defer func() {
		if err := pullMgr.Close(); err != nil {
			t.Logf("Error closing pull manager: %v", err)
		}
	}()

	// ç”¨äºæ”¶é›†æ¥æ”¶åˆ°çš„æ¶ˆæ¯
	var receivedMessages []map[string]interface{}
	var messagesMutex sync.Mutex
	var messageCount int32
	var publishCount int32
	var errorCount int32

	// åˆ›å»ºæµ‹è¯•æ¥æ”¶å™¨
	receiver := &PullReceiver{
		Config: &Config{
			Option: Option{
				Exchange: testExchange,
				Queue:    testQueue,
				Router:   testRouter,
				SigKey:   "rabbitmq_secret_key_32_bytes_1234567890",
				Durable:  true,
			},
			IsNack: false,
		},
		Callback: func(msg *MsgData) error {
			// è§£ææ¶ˆæ¯å†…å®¹
			var content map[string]interface{}
			if err := json.Unmarshal([]byte(msg.Content), &content); err != nil {
				atomic.AddInt32(&errorCount, 1)
				t.Errorf("Failed to parse message content: %v", err)
				return err
			}

			messagesMutex.Lock()
			receivedMessages = append(receivedMessages, content)
			messagesMutex.Unlock()

			atomic.AddInt32(&messageCount, 1)
			count := atomic.LoadInt32(&messageCount)

			t.Logf("ğŸ“¨ Received message #%d: id=%v, phase=%v, publisher=%v",
				count, content["id"], content["phase"], content["publisher"])

			// æ¨¡æ‹Ÿå¤„ç†æ—¶é—´
			time.Sleep(10 * time.Millisecond)
			return nil
		},
	}

	// åˆå§‹åŒ–æ¥æ”¶å™¨
	receiver.initDefaults()
	receiver.initControlChans()

	// æ·»åŠ æ¥æ”¶å™¨åˆ°ç®¡ç†å™¨
	err = pullMgr.AddPullReceiver(receiver)
	if err != nil {
		t.Fatalf("Failed to add receiver: %v", err)
	}

	ctx := context.Background()

	// ç¬¬ä¸€é˜¶æ®µï¼šå¹¶å‘åˆ›å»ºèµ„æºå’Œåˆå§‹æ¶ˆæ¯æµ
	t.Run("ConcurrentResourceCreationAndInitialFlow", func(t *testing.T) {
		t.Log("ğŸš€ Phase 1: Concurrent resource creation and initial message flow")

		// å¹¶å‘å¯åŠ¨å‘å¸ƒå’Œæ¶ˆè´¹æ“ä½œ
		var wg sync.WaitGroup
		wg.Add(2)

		// æ¶ˆè´¹è€…goroutine
		go func() {
			defer wg.Done()
			t.Log("ğŸ“¥ Consumer: Starting consumer operations")

			// ç­‰å¾…ä¸€å°æ®µæ—¶é—´ç¡®ä¿æ¥æ”¶å™¨å¯åŠ¨
			time.Sleep(500 * time.Millisecond)

			// å®šæœŸæ£€æŸ¥æ¥æ”¶çŠ¶æ€
			for i := 0; i < 30; i++ {
				if atomic.LoadInt32(&messageCount) > 0 {
					t.Logf("ğŸ“¥ Consumer: Successfully receiving messages")
					return
				}
				time.Sleep(100 * time.Millisecond)
			}
			t.Logf("ğŸ“¥ Consumer: No messages received yet (this is normal during startup)")
		}()

		// å‘å¸ƒè€…goroutine
		go func() {
			defer wg.Done()
			t.Log("ğŸ“¤ Publisher: Starting concurrent publish operations")

			// å¹¶å‘å‘å¸ƒå¤šæ¡æ¶ˆæ¯ï¼Œæµ‹è¯•èµ„æºæŠ¢å åˆ›å»º
			var pubWg sync.WaitGroup
			for i := 0; i < 5; i++ {
				pubWg.Add(1)
				go func(seq int) {
					defer pubWg.Done()

					testData := map[string]interface{}{
						"id":        fmt.Sprintf("concurrent-init-%d", seq),
						"message":   fmt.Sprintf("Concurrent init message %d", seq),
						"timestamp": time.Now().Unix(),
						"phase":     "concurrent_init",
						"publisher": fmt.Sprintf("goroutine-%d", seq),
						"sequence":  seq,
					}
					testDataBytes, _ := json.Marshal(testData)

					// ä½¿ç”¨ä¸åŒçš„è·¯ç”±é”®æµ‹è¯•å¹¶å‘åˆ›å»º
					router := fmt.Sprintf("%s.%d", testRouter, seq)
					err := pubMgr.Publish(ctx, testExchange, testQueue, 1, string(testDataBytes), WithRouter(router))
					if err != nil {
						atomic.AddInt32(&errorCount, 1)
						t.Errorf("ğŸ“¤ Failed to publish concurrent init message %d: %v", seq, err)
						return
					}

					atomic.AddInt32(&publishCount, 1)
					t.Logf("ğŸ“¤ Published concurrent init message %d successfully", seq)

					// éšæœºå»¶è¿Ÿï¼Œæ¨¡æ‹ŸçœŸå®å¹¶å‘åœºæ™¯
					time.Sleep(time.Duration(rand.Intn(100)) * time.Millisecond)
				}(i + 1)
			}

			pubWg.Wait()
			t.Logf("ğŸ“¤ Publisher: Completed all concurrent publish operations")
		}()

		// ç­‰å¾…æ‰€æœ‰å¹¶å‘æ“ä½œå®Œæˆ
		wg.Wait()

		// éªŒè¯åˆå§‹é˜¶æ®µç»“æœ
		finalPubCount := atomic.LoadInt32(&publishCount)
		finalMsgCount := atomic.LoadInt32(&messageCount)
		finalErrCount := atomic.LoadInt32(&errorCount)

		t.Logf("ğŸ“Š Phase 1 Results: Published: %d, Received: %d, Errors: %d",
			finalPubCount, finalMsgCount, finalErrCount)

		// åˆå§‹é˜¶æ®µåº”è¯¥æˆåŠŸå‘å¸ƒæ¶ˆæ¯
		if finalPubCount == 0 {
			t.Error("No messages were published in initial phase")
		}

		// å…è®¸æ¥æ”¶æ¶ˆæ¯æ•°é‡ä¸å‘å¸ƒä¸å®Œå…¨ä¸€è‡´ï¼ˆç½‘ç»œå»¶è¿Ÿç­‰å› ç´ ï¼‰
		if finalErrCount > finalPubCount/2 {
			t.Errorf("Too many publish errors: %d out of %d", finalErrCount, finalPubCount)
		}
	})

	// ç¬¬äºŒé˜¶æ®µï¼šæŒç»­å¹¶å‘æ“ä½œ
	t.Run("ContinuousConcurrentOperations", func(t *testing.T) {
		t.Log("ğŸ”„ Phase 2: Continuous concurrent operations")

		// é‡ç½®è®¡æ•°å™¨
		atomic.StoreInt32(&publishCount, 0)
		atomic.StoreInt32(&errorCount, 0)

		// å¯åŠ¨æŒç»­çš„å‘å¸ƒgoroutine
		publishDone := make(chan bool)
		go func() {
			defer close(publishDone)
			for i := 0; i < 10; i++ {
				testData := map[string]interface{}{
					"id":        fmt.Sprintf("continuous-%d", i+1),
					"message":   fmt.Sprintf("Continuous operation message %d", i+1),
					"timestamp": time.Now().Unix(),
					"phase":     "continuous_ops",
					"publisher": "continuous-goroutine",
					"sequence":  i + 1,
				}
				testDataBytes, _ := json.Marshal(testData)

				err := pubMgr.Publish(ctx, testExchange, testQueue, 1, string(testDataBytes))
				if err != nil {
					atomic.AddInt32(&errorCount, 1)
					t.Logf("ğŸ“¤ Continuous publish error %d: %v", i+1, err)
				} else {
					atomic.AddInt32(&publishCount, 1)
					t.Logf("ğŸ“¤ Continuous publish success %d", i+1)
				}

				// æ§åˆ¶å‘å¸ƒé¢‘ç‡
				time.Sleep(50 * time.Millisecond)
			}
		}()

		// ç­‰å¾…å‘å¸ƒå®Œæˆ
		<-publishDone

		// ç­‰å¾…ä¸€å°æ®µæ—¶é—´ç¡®ä¿æ‰€æœ‰æ¶ˆæ¯éƒ½è¢«æ¶ˆè´¹
		time.Sleep(2 * time.Second)

		finalPubCount := atomic.LoadInt32(&publishCount)
		finalMsgCount := atomic.LoadInt32(&messageCount)
		finalErrCount := atomic.LoadInt32(&errorCount)

		t.Logf("ğŸ“Š Phase 2 Results: Published: %d, Received: %d, Errors: %d",
			finalPubCount, finalMsgCount, finalErrCount)

		// éªŒè¯æŒç»­æ“ä½œçš„ç»“æœ
		if finalPubCount < 8 { // å…è®¸1-2ä¸ªå¤±è´¥
			t.Errorf("Too few successful publishes: %d/10", finalPubCount)
		}
	})

	// ç¬¬ä¸‰é˜¶æ®µï¼šåˆ†åˆ«æ–­å¼€å‘å¸ƒå’Œæ¶ˆè´¹è¿æ¥
	t.Run("SeparateConnectionDisruptions", func(t *testing.T) {
		t.Log("ğŸ”Œ Phase 3: Separate connection disruptions")

		// å…ˆæ–­å¼€å‘å¸ƒè¿æ¥
		t.Run("DisconnectPublishFirst", func(t *testing.T) {
			t.Log("ğŸ”Œ Disconnecting publish connection first")

			pubMgr.mu.Lock()
			if pubMgr.conn != nil {
				pubMgr.conn.Close()
				pubMgr.conn = nil
			}
			pubMgr.mu.Unlock()

			t.Log("ğŸ“¤ Publish connection closed")
			time.Sleep(1 * time.Second)
		})

		// å†æ–­å¼€æ¶ˆè´¹è¿æ¥
		t.Run("DisconnectPullAfter", func(t *testing.T) {
			t.Log("ğŸ”Œ Disconnecting pull connection after")

			pullMgr.mu.Lock()
			if pullMgr.conn != nil {
				pullMgr.conn.Close()
				pullMgr.conn = nil
			}
			pullMgr.mu.Unlock()

			t.Log("ğŸ“¥ Pull connection closed")
			time.Sleep(1 * time.Second)
		})
	})

	// ç¬¬å››é˜¶æ®µï¼šé‡è¿åçš„å¹¶å‘æ“ä½œéªŒè¯
	t.Run("PostReconnectionConcurrentOperations", func(t *testing.T) {
		t.Log("ğŸ”„ Phase 4: Post-reconnection concurrent operations")

		// ç­‰å¾…é‡è¿å®Œæˆ
		t.Log("â³ Waiting for both connections to be restored...")
		maxWaitTime := 60 * time.Second
		reconnectTimeout := time.After(maxWaitTime)

		for {
			select {
			case <-reconnectTimeout:
				t.Fatalf("Connections not restored within %v", maxWaitTime)

			default:
				pubMgr.mu.RLock()
				pubReady := pubMgr.conn != nil && !pubMgr.conn.IsClosed()
				pubMgr.mu.RUnlock()

				pullMgr.mu.RLock()
				pullReady := pullMgr.conn != nil && !pullMgr.conn.IsClosed()
				pullMgr.mu.RUnlock()

				if pubReady && pullReady {
					t.Log("âœ… Both connections successfully restored!")
					goto connectionsReady
				}

				t.Logf("â³ Waiting... Publish: %v, Pull: %v", pubReady, pullReady)
				time.Sleep(2 * time.Second)
			}
		}

	connectionsReady:
		// é‡è¿åå¹¶å‘å‘å¸ƒæ¶ˆæ¯
		var wg sync.WaitGroup
		wg.Add(1)

		initialMsgCount := atomic.LoadInt32(&messageCount)

		go func() {
			defer wg.Done()
			t.Log("ğŸ“¤ Starting post-reconnection publish operations")

			for i := 0; i < 5; i++ {
				testData := map[string]interface{}{
					"id":        fmt.Sprintf("post-reconnect-%d", i+1),
					"message":   fmt.Sprintf("Post-reconnection message %d", i+1),
					"timestamp": time.Now().Unix(),
					"phase":     "post_reconnect",
					"publisher": "post-reconnect-goroutine",
					"sequence":  i + 1,
				}
				testDataBytes, _ := json.Marshal(testData)

				// ä½¿ç”¨ä¸åŒçš„è·¯ç”±é”®æµ‹è¯•é€šé“é‡å»º
				router := fmt.Sprintf("%s.post.%d", testRouter, i+1)
				err := pubMgr.Publish(ctx, testExchange, testQueue, 1, string(testDataBytes), WithRouter(router))
				if err != nil {
					t.Errorf("ğŸ“¤ Failed to publish post-reconnect message %d: %v", i+1, err)
				} else {
					t.Logf("ğŸ“¤ Successfully published post-reconnect message %d", i+1)
				}

				time.Sleep(100 * time.Millisecond)
			}
		}()

		wg.Wait()

		// ç­‰å¾…æ¶ˆæ¯è¢«æ¶ˆè´¹
		time.Sleep(3 * time.Second)

		finalMsgCount := atomic.LoadInt32(&messageCount)
		postReconnectMsgs := finalMsgCount - initialMsgCount

		t.Logf("ğŸ“Š Post-reconnection results: New messages received: %d", postReconnectMsgs)

		if postReconnectMsgs == 0 {
			t.Error("No messages received after reconnection")
		} else if postReconnectMsgs < 3 { // å…è®¸ä¸€äº›æ¶ˆæ¯ä¸¢å¤±
			t.Logf("âš ï¸  Only %d messages received after reconnection (some loss is acceptable)", postReconnectMsgs)
		} else {
			t.Logf("âœ… Post-reconnection messaging working correctly: %d messages processed", postReconnectMsgs)
		}
	})

	// ç¬¬äº”é˜¶æ®µï¼šæœ€ç»ˆéªŒè¯
	t.Run("FinalValidation", func(t *testing.T) {
		t.Log("ğŸ Phase 5: Final validation")

		// å¥åº·æ£€æŸ¥
		pubHealthy := pubMgr.HealthCheck()
		pullHealthy := pullMgr.HealthCheck()

		if pubHealthy != nil {
			t.Errorf("ğŸ“¤ Publish manager health check failed: %v", pubHealthy)
		} else {
			t.Log("âœ… Publish manager health check passed")
		}

		if pullHealthy != nil {
			t.Errorf("ğŸ“¥ Pull manager health check failed: %v", pullHealthy)
		} else {
			t.Log("âœ… Pull manager health check passed")
		}

		if !receiver.IsHealthy() {
			t.Error("ğŸ“¥ Receiver is not healthy")
		} else {
			t.Log("âœ… Receiver health check passed")
		}

		// ç»Ÿè®¡ä¿¡æ¯
		totalPublished := atomic.LoadInt32(&publishCount)
		totalReceived := atomic.LoadInt32(&messageCount)
		totalErrors := atomic.LoadInt32(&errorCount)

		t.Logf("ğŸ“Š Final Statistics:")
		t.Logf("   ğŸ“¤ Total published: %d", totalPublished)
		t.Logf("   ğŸ“¨ Total received: %d", totalReceived)
		t.Logf("   âŒ Total errors: %d", totalErrors)
		t.Logf("   ğŸ“Š Success rate: %.1f%%", float64(totalReceived)/float64(totalPublished)*100)

		// æœ€ç»ˆæ–­è¨€
		if totalReceived == 0 {
			t.Error("No messages were successfully processed")
		}

		if totalErrors > totalPublished/2 {
			t.Errorf("Error rate too high: %d/%d", totalErrors, totalPublished)
		}
	})

	totalMessages := atomic.LoadInt32(&messageCount)
	t.Logf("ğŸ‰ Concurrent publish/pull operations test completed! Total messages processed: %d", totalMessages)
}

// TestReconnectionLogic ä¸“é—¨æµ‹è¯•é‡è¿é€»è¾‘çš„æ ¸å¿ƒæœºåˆ¶
func TestReconnectionLogic(t *testing.T) {
	// æ¸…ç†å…¨å±€çŠ¶æ€
	pullMgrMu.Lock()
	pullMgrs = make(map[string]*PullManager)
	pullMgrMu.Unlock()

	conf := AmqpConfig{
		DsName:   "test_reconnect_logic",
		Host:     "localhost",
		Port:     5672,
		Username: "guest",
		Password: "guest",
	}

	// åˆ›å»ºç®¡ç†å™¨ä½†ä¸å»ºç«‹è¿æ¥
	mgr := &PullManager{
		conf:      conf,
		connErr:   make(chan *amqp.Error, 1),
		closeChan: make(chan struct{}),
		receivers: make([]*PullReceiver, 0),
	}

	// æ‰‹åŠ¨è®¾ç½®ä¸€ä¸ªæ¨¡æ‹Ÿçš„è¿æ¥ï¼ˆæˆ‘ä»¬ä¸ä¼šçœŸæ­£è¿æ¥ï¼‰
	mgr.conn = &amqp.Connection{} // åªæ˜¯ä¸ºäº†æµ‹è¯•é€»è¾‘

	// æµ‹è¯•è¿æ¥ç›‘æ§çš„è®¾ç½®
	mgr.mu.Lock()
	monitorStarted := make(chan bool, 1)
	go func() {
		// æ¨¡æ‹Ÿ monitorConnection çš„æ ¸å¿ƒé€»è¾‘
		defer func() {
			monitorStarted <- true
		}()

		mgr.mu.RLock()
		if mgr.conn == nil || mgr.closed {
			mgr.mu.RUnlock()
			return
		}

		conn := mgr.conn
		closeChan := make(chan *amqp.Error, 1) // æ¨¡æ‹Ÿè¿æ¥å…³é—­é€šçŸ¥
		mgr.mu.RUnlock()

		// æ¨¡æ‹Ÿè¿æ¥æ–­å¼€
		go func() {
			time.Sleep(100 * time.Millisecond)
			closeChan <- &amqp.Error{Code: 320, Reason: "Connection forced: test"}
		}()

		select {
		case <-mgr.closeChan:
			return
		case err := <-closeChan:
			if err != nil {
				t.Logf("Simulated connection error: %v", err)

				// éªŒè¯è¿æ¥æ˜¯å¦ä»ç„¶æ˜¯åŒä¸€ä¸ª
				mgr.mu.RLock()
				isSameConnection := (mgr.conn == conn)
				mgr.mu.RUnlock()

				if isSameConnection {
					t.Log("Connection is the same, should trigger reconnection")
					// åœ¨å®é™…ä»£ç ä¸­ï¼Œè¿™é‡Œä¼šè°ƒç”¨ reconnectAllReceivers()
				}
			}
		}
	}()
	mgr.mu.Unlock()

	// ç­‰å¾…ç›‘æ§goroutineå¯åŠ¨
	select {
	case <-monitorStarted:
		t.Log("Connection monitoring logic started successfully")
	case <-time.After(1 * time.Second):
		t.Error("Connection monitoring did not start within timeout")
	}

	// éªŒè¯ç®¡ç†å™¨çš„åŸºæœ¬çŠ¶æ€
	assert.NotNil(t, mgr.conf)
	assert.Equal(t, "test_reconnect_logic", mgr.conf.DsName)
	assert.NotNil(t, mgr.closeChan)

	t.Log("Reconnection logic test completed successfully")
}

// TestRestartReceiversLogic æµ‹è¯•æ¥æ”¶å™¨é‡å¯é€»è¾‘
func TestRestartReceiversLogic(t *testing.T) {
	conf := AmqpConfig{
		DsName:   "test_restart",
		Host:     "localhost",
		Port:     5672,
		Username: "guest",
		Password: "guest",
	}

	mgr := &PullManager{
		conf:      conf,
		connErr:   make(chan *amqp.Error, 1),
		closeChan: make(chan struct{}),
		receivers: make([]*PullReceiver, 0),
	}

	// åˆ›å»ºå‡ ä¸ªæµ‹è¯•æ¥æ”¶å™¨
	receiver1 := &PullReceiver{
		Config: &Config{
			Option: Option{Queue: "queue1"},
		},
	}
	receiver2 := &PullReceiver{
		Config: &Config{
			Option: Option{Queue: "queue2"},
		},
	}

	mgr.receivers = append(mgr.receivers, receiver1, receiver2)

	// æµ‹è¯•é‡å¯é€»è¾‘ï¼ˆä¸å®é™…æ‰§è¡Œgoroutineå¯åŠ¨ï¼‰
	originalReceivers := make([]*PullReceiver, len(mgr.receivers))
	copy(originalReceivers, mgr.receivers)

	// éªŒè¯æ¥æ”¶å™¨åˆ—è¡¨è¢«æ­£ç¡®å¤åˆ¶
	assert.Equal(t, 2, len(originalReceivers))
	assert.Equal(t, "queue1", originalReceivers[0].Config.Option.Queue)
	assert.Equal(t, "queue2", originalReceivers[1].Config.Option.Queue)

	t.Log("Restart receivers logic test completed successfully")
}

// TestReconnectionWorkflow ç«¯åˆ°ç«¯é‡è¿å·¥ä½œæµæµ‹è¯•
func TestReconnectionWorkflow(t *testing.T) {
	// æ¸…ç†å…¨å±€çŠ¶æ€
	pullMgrMu.Lock()
	pullMgrs = make(map[string]*PullManager)
	pullMgrMu.Unlock()

	conf := AmqpConfig{
		DsName:   "test_reconnect_workflow",
		Host:     "localhost",
		Port:     5672,
		Username: "guest",
		Password: "guest",
	}

	// åˆ›å»ºç®¡ç†å™¨
	mgr := &PullManager{
		conf:      conf,
		connErr:   make(chan *amqp.Error, 1),
		closeChan: make(chan struct{}),
		receivers: make([]*PullReceiver, 0),
	}

	// æ·»åŠ ä¸€äº›æµ‹è¯•æ¥æ”¶å™¨
	receiver1 := &PullReceiver{
		Config: &Config{
			Option: Option{Queue: "test_queue_1"},
		},
	}
	receiver2 := &PullReceiver{
		Config: &Config{
			Option: Option{Queue: "test_queue_2"},
		},
	}

	mgr.receivers = append(mgr.receivers, receiver1, receiver2)

	// æµ‹è¯•1: éªŒè¯é‡è¿æ–¹æ³•çš„å­˜åœ¨å’ŒåŸºæœ¬ç»“æ„
	t.Run("ReconnectMethodStructure", func(t *testing.T) {
		// éªŒè¯ reconnectAllReceivers æ–¹æ³•å­˜åœ¨å¹¶ä¸”å¯ä»¥è°ƒç”¨
		// æ³¨æ„ï¼šè¿™ä¸ªæ–¹æ³•ä¼šå°è¯•çœŸå®è¿æ¥ï¼Œæ‰€ä»¥æˆ‘ä»¬åªéªŒè¯å®ƒä¸ä¼španic
		defer func() {
			if r := recover(); r != nil {
				t.Errorf("reconnectAllReceivers panicked: %v", r)
			}
		}()

		// åœ¨ goroutine ä¸­è°ƒç”¨ä»¥é¿å…é˜»å¡æµ‹è¯•
		done := make(chan bool, 1)
		go func() {
			defer func() { done <- true }()
			// è¿™ä¸ªè°ƒç”¨ä¼šå¤±è´¥ï¼Œä½†ä¸åº”è¯¥panic
			mgr.reconnectAllReceivers()
		}()

		// ç­‰å¾…ä¸€å°æ®µæ—¶é—´
		select {
		case <-done:
			t.Log("reconnectAllReceivers completed without panic")
		case <-time.After(2 * time.Second):
			t.Log("reconnectAllReceivers is still running (expected for connection attempts)")
		}
	})

	// æµ‹è¯•2: éªŒè¯æ¥æ”¶å™¨é‡å¯é€»è¾‘
	t.Run("ReceiverRestartLogic", func(t *testing.T) {
		// ä¿å­˜åŸå§‹æ¥æ”¶å™¨çŠ¶æ€
		originalCount := len(mgr.receivers)
		originalQueue1 := mgr.receivers[0].Config.Option.Queue

		// è°ƒç”¨é‡å¯é€»è¾‘ï¼ˆæ¨¡æ‹Ÿï¼‰
		mgr.restartAllReceivers()

		// éªŒè¯æ¥æ”¶å™¨æ•°é‡æ²¡æœ‰æ”¹å˜
		assert.Equal(t, originalCount, len(mgr.receivers))
		assert.Equal(t, originalQueue1, mgr.receivers[0].Config.Option.Queue)

		t.Log("Receiver restart logic works correctly")
	})

	// æµ‹è¯•3: éªŒè¯è¿æ¥ç›‘æ§çš„è®¾ç½®
	t.Run("ConnectionMonitorSetup", func(t *testing.T) {
		// é‡æ–°åˆ›å»ºç®¡ç†å™¨ï¼Œé¿å… WaitGroup é—®é¢˜
		testMgr := &PullManager{
			conf:      conf,
			connErr:   make(chan *amqp.Error, 1),
			closeChan: make(chan struct{}),
			receivers: make([]*PullReceiver, 0),
		}

		// æ¨¡æ‹Ÿè¿æ¥å¯¹è±¡ï¼ˆä¸å®é™…è¿æ¥ï¼‰
		testMgr.conn = &amqp.Connection{}

		// åˆå§‹åŒ– WaitGroup
		testMgr.monitorWg = sync.WaitGroup{}
		testMgr.monitorWg.Add(1)

		// å¯åŠ¨è¿æ¥ç›‘æ§
		monitorDone := make(chan bool, 1)
		go func() {
			defer func() { monitorDone <- true }()
			testMgr.monitorConnection()
		}()

		// ç­‰å¾…ç›‘æ§å¯åŠ¨
		time.Sleep(100 * time.Millisecond)

		// å‘é€å…³é—­ä¿¡å·
		close(testMgr.closeChan)

		// ç­‰å¾…ç›‘æ§é€€å‡º
		select {
		case <-monitorDone:
			t.Log("Connection monitor exited cleanly on close signal")
		case <-time.After(1 * time.Second):
			t.Error("Connection monitor did not exit within timeout")
		}
	})

	// æµ‹è¯•4: éªŒè¯èµ„æºæ¸…ç†
	t.Run("ResourceCleanup", func(t *testing.T) {
		// å…³é—­ç®¡ç†å™¨
		err := mgr.Close()
		if err != nil {
			t.Logf("Close returned error (expected): %v", err)
		}

		// éªŒè¯è¿æ¥è¢«æ¸…ç†
		mgr.mu.RLock()
		conn := mgr.conn
		mgr.mu.RUnlock()

		if conn != nil {
			t.Log("Connection cleanup completed")
		}

		t.Log("Resource cleanup test completed")
	})

	t.Log("Reconnection workflow test completed successfully")
}
